{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how to use DBT to:\n",
    "# 1: train a VGG-like network on CIFAR-10\n",
    "# 2: continue a train from the last iteration\n",
    "# 3: do TRANSFER LEARNING from the trained model to another model that will be able to classify CIFAR-100\n",
    "# 4: do FINE TUNING of the model trained on CIFAR-10 to solve the CIFAR-100 classification problem\n",
    "# 5: compare the train/validation/test performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "from dytb.inputs import Cifar10, Cifar100\n",
    "from dytb.train import train\n",
    "from dytb.models.VGG import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "vgg = VGG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the CIFAR-10 input source\n",
    "cifar10 = Cifar10.Cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-09 10:22:02.488145: step 0, loss = 2.3831 (13.0 examples/sec; 3.850 sec/batch)\n",
      "2017-03-09 10:22:11.150772: step 100, loss = 2.3359 (578.8 examples/sec; 0.086 sec/batch)\n",
      "2017-03-09 10:22:20.163013: step 200, loss = 2.3398 (570.5 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:22:29.187936: step 300, loss = 2.2427 (570.7 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:22:38.141480: step 400, loss = 1.9855 (569.9 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:22:47.100676: step 500, loss = 1.9199 (572.5 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:22:56.063644: step 600, loss = 2.0006 (571.0 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:23:05.067732: step 700, loss = 1.8361 (573.2 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:23:14.003720: step 800, loss = 1.8202 (570.3 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:23:23.006292: step 900, loss = 1.9278 (569.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:23:31.998452: step 1000, loss = 1.7752 (571.4 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:23:37.717787 (1): train accuracy = 0.480 validation accuracy = 0.351\n"
     ]
    }
   ],
   "source": [
    "# 1: Train VGG on Cifar10 for an Epoch\n",
    "\n",
    "# Place the train process on GPU:0\n",
    "device = '/gpu:0'\n",
    "with tf.device(device):\n",
    "    info = train(\n",
    "        model=vgg,\n",
    "        dataset=cifar10,\n",
    "        hyperparameters={\n",
    "            \"epochs\": 1,\n",
    "            \"batch_size\": 50,\n",
    "            \"regularizations\": {\n",
    "                \"l2\": 1e-5,\n",
    "                \"augmentation\": {\n",
    "                    \"name\": \"FlipLR\",\n",
    "                    \"fn\": tf.image.random_flip_left_right\n",
    "                }\n",
    "            },\n",
    "            \"gd\": {\n",
    "                \"optimizer\": tf.train.AdamOptimizer,\n",
    "                \"args\": {\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"beta1\": 0.9,\n",
    "                    \"beta2\": 0.99,\n",
    "                    \"epsilon\": 1e-8\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        force_restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>VGG</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>0.35594</td>\n",
       "      <td>0.3509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset model    test    train  validation\n",
       "0  CIFAR-10   VGG  0.3509  0.35594      0.3509"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info containes every information related to the trained model.\n",
    "# We're interested in stats only, thus we extract only them from the info dict\n",
    "# Display the results in a table. Let's use a Pandas DataFrame for that\n",
    "df = pd.DataFrame.from_records(info[\"stats\"], index=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2: train it again for another epoch\n",
    "# Note the `force_restart` parameter removed.\n",
    "# `epochs` is the TOTAL number of epoch for the trained model\n",
    "# Thus since we trained it before for a single epoch,\n",
    "# we set \"epochs\": 2 in order to train it for another epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-09 10:24:30.244383: step 1100, loss = 1.6141 (569.0 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:24:39.355711: step 1200, loss = 1.5945 (568.4 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:24:48.377192: step 1300, loss = 1.5742 (567.7 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:24:57.380686: step 1400, loss = 1.6840 (566.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:06.408212: step 1500, loss = 1.7028 (568.9 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:15.413003: step 1600, loss = 1.3821 (564.5 examples/sec; 0.089 sec/batch)\n",
      "2017-03-09 10:25:24.401205: step 1700, loss = 1.3084 (567.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:33.378194: step 1800, loss = 1.5210 (566.6 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:42.365851: step 1900, loss = 1.3080 (566.6 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:51.353771: step 2000, loss = 0.9364 (566.4 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:25:57.054625 (2): train accuracy = 0.500 validation accuracy = 0.559\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):\n",
    "    info = train(\n",
    "        model=vgg,\n",
    "        dataset=cifar10,\n",
    "        hyperparameters={\n",
    "            \"epochs\": 2,\n",
    "            \"batch_size\": 50,\n",
    "            \"regularizations\": {\n",
    "                \"l2\": 1e-5,\n",
    "                \"augmentation\": {\n",
    "                    \"name\": \"FlipLR\",\n",
    "                    \"fn\": tf.image.random_flip_left_right\n",
    "                }\n",
    "            },\n",
    "            \"gd\": {\n",
    "                \"optimizer\": tf.train.AdamOptimizer,\n",
    "                \"args\": {\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"beta1\": 0.9,\n",
    "                    \"beta2\": 0.99,\n",
    "                    \"epsilon\": 1e-8\n",
    "                }\n",
    "            }\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>VGG</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.57408</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset model   test    train  validation\n",
       "0  CIFAR-10   VGG  0.559  0.57408       0.559"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results in a table. Let's use a Pandas DataFrame for that\n",
    "df = pd.DataFrame.from_records(info[\"stats\"], index=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save last trained model info\n",
    "vggInfo = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-09 10:26:40.740317: step 0, loss = 4.7881 (18.2 examples/sec; 2.751 sec/batch)\n",
      "2017-03-09 10:26:49.798616: step 100, loss = 4.6482 (571.8 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:26:58.722593: step 200, loss = 4.6415 (570.5 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:27:07.663329: step 300, loss = 4.6426 (567.4 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:27:16.674381: step 400, loss = 4.6374 (572.5 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:27:25.641717: step 500, loss = 4.6342 (568.5 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:27:34.601394: step 600, loss = 4.6304 (570.7 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:27:43.630895: step 700, loss = 4.6243 (567.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:27:52.641419: step 800, loss = 4.6219 (570.7 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:28:01.675475: step 900, loss = 4.6192 (569.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:28:10.648537: step 1000, loss = 4.6115 (567.3 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:28:16.415617 (1): train accuracy = 0.000 validation accuracy = 0.010\n"
     ]
    }
   ],
   "source": [
    "# 3: TRANSFER LEARNING\n",
    "# Use the best model trained on Cifar10, to classify Cifar 100 images.\n",
    "# Thus we train ONLY the softmax linear scope (that has 100 neurons, now),\n",
    "# keeping constant any other previosly trained layer\n",
    "# We load the weights from the previous trained model, or better\n",
    "# DyTB saves the \"best\" model (w.r.t. a metric) in a separate folder\n",
    "# So we extract the info[\"paths\"][\"best\"] path, that's the path of the best\n",
    "# model trained so far.\n",
    "cifar100 = Cifar100.Cifar100()\n",
    "with tf.device(device):\n",
    "    transferInfo = train(\n",
    "        model=vgg,\n",
    "        dataset=cifar100,\n",
    "        hyperparameters={\n",
    "            \"epochs\": 1,\n",
    "            \"batch_size\": 50,\n",
    "            \"regularizations\": {\n",
    "                \"l2\": 1e-5,\n",
    "                \"augmentation\": {\n",
    "                    \"name\": \"FlipLR\",\n",
    "                    \"fn\": tf.image.random_flip_left_right\n",
    "                }\n",
    "            },\n",
    "            \"gd\": {\n",
    "                \"optimizer\": tf.train.AdamOptimizer,\n",
    "                \"args\": {\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"beta1\": 0.9,\n",
    "                    \"beta2\": 0.99,\n",
    "                    \"epsilon\": 1e-8\n",
    "                    }\n",
    "                }\n",
    "        },\n",
    "        force_restart=True,\n",
    "        surgery={\n",
    "            \"checkpoint_path\": vggInfo[\"paths\"][\"best\"],\n",
    "            \"exclude_scopes\": \"VGG/softmax_linear\",\n",
    "            \"trainable_scopes\": \"VGG/softmax_linear\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-09 10:29:00.201507: step 0, loss = 4.6236 (18.2 examples/sec; 2.743 sec/batch)\n",
      "2017-03-09 10:29:09.263967: step 100, loss = 4.6435 (570.5 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:29:18.263172: step 200, loss = 4.6449 (571.9 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:29:27.198371: step 300, loss = 4.6407 (573.9 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:29:36.164529: step 400, loss = 4.6365 (572.2 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:29:45.133434: step 500, loss = 4.6321 (569.9 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:29:54.071067: step 600, loss = 4.6356 (570.5 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:30:03.000208: step 700, loss = 4.6269 (574.1 examples/sec; 0.087 sec/batch)\n",
      "2017-03-09 10:30:11.989766: step 800, loss = 4.6260 (571.1 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:30:20.974848: step 900, loss = 4.6198 (568.8 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:30:30.015285: step 1000, loss = 4.6232 (567.9 examples/sec; 0.088 sec/batch)\n",
      "2017-03-09 10:30:35.924013 (1): train accuracy = 0.020 validation accuracy = 0.010\n"
     ]
    }
   ],
   "source": [
    "# 4: FINE TUNING:\n",
    "# Use the model pointed by vggInfo to fine tune the whole network\n",
    "# and tune it on Cifar100.\n",
    "# Let's retrain the whole network end-to-end, starting from the learned weights\n",
    "# Just remove the \"traiable_scopes\" section from the surgery parameter\n",
    "with tf.device(device):\n",
    "    fineTuningInfo = train(\n",
    "        model=vgg,\n",
    "        dataset=cifar100,\n",
    "        hyperparameters={\n",
    "            \"epochs\": 1,\n",
    "            \"batch_size\": 50,\n",
    "            \"regularizations\": {\n",
    "                \"l2\": 1e-5,\n",
    "                \"augmentation\": {\n",
    "                    \"name\": \"FlipLR\",\n",
    "                    \"fn\": tf.image.random_flip_left_right\n",
    "                }\n",
    "            },\n",
    "            \"gd\": {\n",
    "                \"optimizer\": tf.train.AdamOptimizer,\n",
    "                \"args\": {\n",
    "                    \"learning_rate\": 1e-3,\n",
    "                    \"beta1\": 0.9,\n",
    "                    \"beta2\": 0.99,\n",
    "                    \"epsilon\": 1e-8\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        force_restart=True,\n",
    "        surgery={\n",
    "            \"checkpoint_path\": vggInfo[\"paths\"][\"best\"],\n",
    "            \"exclude_scopes\": \"VGG/softmax_linear\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>VGG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01024</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset model  test    train  validation\n",
       "0  CIFAR-100   VGG  0.01  0.01024        0.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the performance of Transfer learning and Fine Tuning\n",
    "df = pd.DataFrame.from_records(transferInfo[\"stats\"], index=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR-100</td>\n",
       "      <td>VGG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01038</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset model  test    train  validation\n",
       "0  CIFAR-100   VGG  0.01  0.01038        0.01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(fineTuningInfo[\"stats\"], index=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'args': {   'batch_size': 50,\n",
      "                'checkpoint_path': '',\n",
      "                'comment': '',\n",
      "                'dataset': <dytb.inputs.Cifar10.Cifar10 object at 0x7f896c19a1d0>,\n",
      "                'epochs': 2,\n",
      "                'exclude_scopes': '',\n",
      "                'force_restart': False,\n",
      "                'gd': {   'args': {   'beta1': 0.9,\n",
      "                                      'beta2': 0.99,\n",
      "                                      'epsilon': 1e-08,\n",
      "                                      'learning_rate': 0.001},\n",
      "                          'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>},\n",
      "                'lr_decay': {'enabled': False, 'epochs': 25, 'factor': 0.1},\n",
      "                'model': <dytb.models.VGG.VGG object at 0x7f896c19a128>,\n",
      "                'regularizations': {   'augmentation': <function random_flip_left_right at 0x7f89109cb0d0>,\n",
      "                                       'l2': 1e-05},\n",
      "                'trainable_scopes': ''},\n",
      "    'paths': {   'best': '/mnt/data/pgaleone/dytb_work/examples/log/VGG/CIFAR-10_Adam_l2=1e-05_fliplr/best',\n",
      "                 'current': '/mnt/data/pgaleone/dytb_work/examples',\n",
      "                 'log': '/mnt/data/pgaleone/dytb_work/examples/log/VGG/CIFAR-10_Adam_l2=1e-05_fliplr'},\n",
      "    'stats': {   'dataset': 'CIFAR-10',\n",
      "                 'model': 'VGG',\n",
      "                 'test': 0.55899998381733895,\n",
      "                 'train': 0.5740799830555916,\n",
      "                 'validation': 0.55899998381733895},\n",
      "    'steps': {'decay': 25000, 'epoch': 1000, 'log': 100, 'max': 2000}}\n"
     ]
    }
   ],
   "source": [
    "# For completeness, lets see what a info object contains\n",
    "pprint.pprint(info, indent=4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
